{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magyar NLP - előfeldolgozás II."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az NLP bevezető II. notebookban kiszűrtük a szövegből a zajokat, valamint létrehoztuk a header és a szövegtörsz összefűzéséből a valódi szövegtörzset.\n",
    "<br>\n",
    "Vannak olyan lépések, amelyeket el kell végeznünk mielőtt nekiállnánk elemezni a dokumentumgyűjteményünket.\n",
    "<br>\n",
    "Ebben a notebook-ban sorrol sorra átvesszük, hogy milyen szövegelemzési előfeldolgozáson kell alávetni a dokomentumgyűjteményt, hogy elemezhető legyen például szózsák modellekkel.\n",
    "<br>\n",
    "**Tartalom:**\n",
    "* Csomagok importálása,\n",
    "* Fájlok beolvasása,\n",
    "* Tokenizáció,\n",
    "* Lemmatizáció,\n",
    "* Part of Speach (PoS) tagging\n",
    "* Szűrés lemmákon\n",
    "* Szűrés a tisztított szövegen\n",
    "* Szűrt korpusz exportálása"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Csomagok importálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adatkezelés\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP csomagok:\n",
    "import spacy\n",
    "import hu_core_ud_lg as hu\n",
    "nlp = hu.load()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "tknzr = TweetTokenizer() # gyorsabb, mint a Spacy tokenizáció\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle fájl beolvasása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index = pd.read_pickle(\"2020_corpus_preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_index.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fölösleges oszlopok eldobása**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index = df_index.drop(['head', 'szoveg'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizáció\n",
    "Ahhoz, hogy az elemzési egységeket meg tudjuk határozni, a dokumentumok szövegét\n",
    "tokenizálni kell. Tokennek tekinthető például minden egyes szó egy dokumentumban. “Ez az\n",
    "alma zöld” mondat 4 tokenből, azaz szóból áll. A tokenizálás sokszor bonyolult feladat\n",
    "lehet, ha a mondatokban a szavak nem csak szóközökkel vannak elválasztva. A tokenizálás\n",
    "következtében előáll egy lista a dokumentum szavaiból. Az egyedi szavak leválogatásával\n",
    "állítható elő a szótár."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index[\"merged_cleaned_tokens\"] = df_index[\"merged_cleaned\"].apply(lambda x: tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index[['merged_cleaned', 'merged_cleaned_tokens']][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizáció\n",
    "A toldaléklevágás másik módszere a lemmatizálás. A lemmatizálás a nyelvészetben használt\n",
    "fogalom, és a szó lemmájának, azaz normalizált vagy szótári alakjának megkeresését\n",
    "szolgálja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index[\"merged_cleaned_lemmas\"] = df_index['merged_cleaned'].apply(lambda x: [i.lemma_ for i in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index[['merged_cleaned_tokens', 'merged_cleaned_lemmas']][17:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speach (PoS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index['merged_cleaned_ners'] = df_index['merged_cleaned'].apply(lambda x: [i.pos_ for i in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index[['merged_cleaned_tokens', 'merged_cleaned_lemmas', 'merged_cleaned_ners']][20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020-as NLP feldolgozott korpusz exportálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index.to_pickle('2020_nlp_preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dokumentumgyűjtemény szűrése lemmákon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feladat**: A 2020-as index.hu-s cikkek közül gyűjtsük le azokat, amelyek a koronavírussal kapcsolatosak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. lépés**: Hozzuk létre a keresendő szavak listáját"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korona = ['vírus', 'pandémia', 'covid', 'koronavírus', 'oltás', 'vakcina', 'karantén']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. lépés**: Csináljunk koronavírus flag-et, ezzel jelöljük, hogy van-e a szövegben kapcsolódó szó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index['korona_flg'] = df_index[\"merged_cleaned_lemmas\"].apply(lambda x: 1 if any(i in x for i in korona) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. lépés**: A flag alapján hozzunk létre egy szűrt adatbázist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_korona = df_index[df_index['korona_flg'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Koronavírushoz köthető cikkek száma a dokumentumgyűjteményben: {} db\".format(len(df_korona)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szűrés a tisztított szövegen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha például el akarjuk távolítani azokat, amelyek szövegében szerepel az operatív törzs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_korona_no_operativ = df_korona[~df_korona['merged_cleaned'].str.contains('operatív törzs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_korona_no_operativ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vagy ha egy lista alapján akarunk szűrni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kell = ['operatív', 'magyar', 'vakcina']\n",
    "df3 = df_korona[df_korona['merged_cleaned'].str.contains(r'\\b(?:{})\\b'.format('|'.join(kell)))] \n",
    "len(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Szűrt dokumentumgyűjtemény exportálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_korona.to_pickle('2020_korona_corpus')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
